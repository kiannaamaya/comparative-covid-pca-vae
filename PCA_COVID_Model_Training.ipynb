{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9UFWopnVdZe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9UFWopnVdZe",
        "outputId": "df742c5a-f7a4-4f11-bd82-bcf7f1da1682"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1nSKLSL5VLXc",
      "metadata": {
        "id": "1nSKLSL5VLXc"
      },
      "outputs": [],
      "source": [
        "!pip install shap\n",
        "!pip install feature_engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BkeztY2BYnL-",
      "metadata": {
        "id": "BkeztY2BYnL-"
      },
      "outputs": [],
      "source": [
        "!pip install lime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GK3igIEsoMyD",
      "metadata": {
        "id": "GK3igIEsoMyD"
      },
      "outputs": [],
      "source": [
        "!pip install -U imbalanced-learn scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c39a0b7f",
      "metadata": {
        "id": "c39a0b7f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.utils import class_weight\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from scipy.stats.mstats import winsorize\n",
        "from scipy.stats import skew\n",
        "from sklearn.utils import shuffle\n",
        "from pandas.api.types import CategoricalDtype\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from statsmodels.tools.tools import add_constant\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import PowerTransformer, StandardScaler\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from feature_engine.selection import RecursiveFeatureAddition\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.over_sampling import BorderlineSMOTE\n",
        "import shap\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DOdsvKY6Cp-M",
      "metadata": {
        "id": "DOdsvKY6Cp-M"
      },
      "outputs": [],
      "source": [
        "random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c93be11",
      "metadata": {
        "id": "9c93be11"
      },
      "source": [
        "I am utilizing this code as guidance, https://github.com/inab-certh/Predicting-COVID-19-severity-through-interpretable-AI-analysis-of-plasma-proteomics/blob/main/Task%202.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9251da06",
      "metadata": {
        "id": "9251da06"
      },
      "source": [
        "Categorizing ages as follows:\n",
        "- ages 0 to 40\n",
        "- ages 41 to 60\n",
        "- ages 61 to 80\n",
        "- ages 81 and above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MHr3YQu4H3tR",
      "metadata": {
        "id": "MHr3YQu4H3tR"
      },
      "outputs": [],
      "source": [
        "def bin_age(data):\n",
        "  age_bins = [0, 40, 60, 80, 100]\n",
        "  age_labels = [0, 1, 2, 3]\n",
        "\n",
        "  data['age'] = pd.cut(data['age'], bins=age_bins, labels=age_labels, right=True).astype(int)\n",
        "\n",
        "  print(\"ISB Clinical:\")\n",
        "  print(data['age'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "h2YsYpbyI1hN",
      "metadata": {
        "id": "h2YsYpbyI1hN"
      },
      "outputs": [],
      "source": [
        "def target_recode_scale(value):\n",
        "    if value in [1, 2, 3, '1 or 2']:\n",
        "        return 'mild'\n",
        "    elif value in [4, 5]:\n",
        "        return 'moderate'\n",
        "    elif value in [6, 7]:\n",
        "        return 'severe'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6v6Jo73zNvZ2",
      "metadata": {
        "id": "6v6Jo73zNvZ2"
      },
      "outputs": [],
      "source": [
        "def replace_unknown_with_mode(train_column, test_column):\n",
        "    mode_value = train_column.mode()[0]\n",
        "    return train_column.replace('Unknown', mode_value), test_column.replace('Unknown', mode_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SiZ7DhozoeTk",
      "metadata": {
        "id": "SiZ7DhozoeTk"
      },
      "outputs": [],
      "source": [
        "def get_data_type(data_path, file_type='excel'):\n",
        "    if 'clinical' in data_path.lower():\n",
        "        data = pd.read_excel(data_path, keep_default_na=False, na_values=['NA'])\n",
        "    else:\n",
        "        if file_type == 'csv':\n",
        "            data = pd.read_csv(data_path, index_col=0)\n",
        "        elif file_type == 'excel':\n",
        "            data = pd.read_excel(data_path)\n",
        "        elif file_type == 'txt':\n",
        "            data = pd.read_csv(data_path, sep='\\t', index_col=0)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported file type\")\n",
        "    print(\"Data loaded successfully.\")\n",
        "\n",
        "    if 'clinical' in data_path.lower():\n",
        "        data_type = 'Clinical'\n",
        "    elif 'metabolomics' in data_path.lower():\n",
        "        data_type = 'Metabolomics'\n",
        "    elif 'proteomics' in data_path.lower():\n",
        "        data_type = 'Proteomics'\n",
        "    else:\n",
        "        raise ValueError(\"Unknown data type\")\n",
        "\n",
        "    print(data.columns)\n",
        "    return data, data_type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "x3Gk92YAqwrZ",
      "metadata": {
        "id": "x3Gk92YAqwrZ"
      },
      "outputs": [],
      "source": [
        "def preprocess_clinical(data, output_dir):\n",
        "  data.drop('ethnicity', axis=1, inplace=True)\n",
        "  data.set_index('subject_id', inplace=True)\n",
        "  relevant_clinical_columns = ['who_severity', 'sex', 'age', 'cigarette_smoking', 'kidney_disease',\n",
        "                            'chronic_hypertension', 'cancer', 'asthma', 'copd', 'coronary_artery_disease',\n",
        "                            'respiratory_support']\n",
        "\n",
        "  data = data[relevant_clinical_columns]\n",
        "  print(\"ISB Clinical columns: \", data)\n",
        "\n",
        "  data['respiratory_support'].replace({'None': 'None'}, inplace=True)\n",
        "  print(\"These are the unique respiratory support values:\", data['respiratory_support'].unique())\n",
        "  clin_rows_with_na = data.isna().any(axis=1)\n",
        "\n",
        "  print(\"Number of rows with at least one missing value:\")\n",
        "  print(\"ISB Clinical: \", clin_rows_with_na.sum())\n",
        "\n",
        "  bin_age(data)\n",
        "  data['who_severity'] = data['who_severity'].apply(target_recode_scale)\n",
        "  print(\"ISB Clinical Severity Value Counts: \", data['who_severity'].value_counts())\n",
        "\n",
        "  y = data['who_severity'].map({'mild': 0, 'moderate': 1, 'severe': 2})\n",
        "  print(\"Target:\", y.value_counts())\n",
        "\n",
        "  X = data.drop(['who_severity'], axis = 1)\n",
        "\n",
        "  if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "  joblib.dump(X, os.path.join(output_dir, 'X_clinical.pkl'))\n",
        "\n",
        "  return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "N4Xdx8KbsEuf",
      "metadata": {
        "id": "N4Xdx8KbsEuf"
      },
      "outputs": [],
      "source": [
        "def preprocess_metabolomics(data, output_dir):\n",
        "  data.set_index('subject_id', inplace=True)\n",
        "  met_rows_with_na = data.isna().any(axis=1)\n",
        "\n",
        "  print(\"Number of rows with at least one missing value:\")\n",
        "  print(\"ISB Metabolomics: \", met_rows_with_na.sum())\n",
        "  X = data.drop(['Blood Draw', 'Healthy or INCOV', 'age', 'sex', 'BMI'], axis = 1)\n",
        "\n",
        "  if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "  joblib.dump(X, os.path.join(output_dir, 'X_metabolomics.pkl'))\n",
        "\n",
        "  return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "td0Cbo2-saXS",
      "metadata": {
        "id": "td0Cbo2-saXS"
      },
      "outputs": [],
      "source": [
        "def preprocess_proteomics(data, output_dir):\n",
        "  data.set_index('subject_id', inplace=True)\n",
        "  pro_rows_with_na = data.isna().any(axis=1)\n",
        "\n",
        "  print(\"Number of rows with at least one missing value:\")\n",
        "  print(\"ISB Proteomics: \", pro_rows_with_na.sum())\n",
        "  X = data.drop(['Blood Draw', 'Healthy or INCOV', 'age', 'sex', 'BMI'], axis = 1)\n",
        "\n",
        "  if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "  joblib.dump(X, os.path.join(output_dir, 'X_proteomics.pkl'))\n",
        "\n",
        "  return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "t5vHCqj1uynF",
      "metadata": {
        "id": "t5vHCqj1uynF"
      },
      "outputs": [],
      "source": [
        "def data_split(X, y, data_type):\n",
        "  X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "  X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
        "  print(f\"{data_type} Train Target:\", y_train.value_counts(normalize=True))\n",
        "  return X_train, X_test, X_val, y_test, y_test, y_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZryxcouKHZWV",
      "metadata": {
        "id": "ZryxcouKHZWV"
      },
      "outputs": [],
      "source": [
        "def save_data_splits(X_train, X_test, X_val, y_train, y_test, y_val, output_dir, X_train_resampled=None, y_train_resampled=None):\n",
        "    \"\"\"\n",
        "    Saves the train-test data splits to the specified directory using joblib.\n",
        "\n",
        "    Args:\n",
        "    - X_train (pd.DataFrame or np.ndarray): Training data features.\n",
        "    - X_test (pd.DataFrame or np.ndarray): Testing data features.\n",
        "    - y_train (pd.Series or np.ndarray): Training data labels.\n",
        "    - y_test (pd.Series or np.ndarray): Testing data labels.\n",
        "    - output_dir (str): Directory path where the data splits will be saved.\n",
        "\n",
        "    Note:\n",
        "    - If the output directory does not exist, it will be created.\n",
        "    \"\"\"\n",
        "\n",
        "    assert X_train.shape[0] == y_train.shape[0], \"Mismatch in training data and labels.\"\n",
        "    assert X_test.shape[0] == y_test.shape[0], \"Mismatch in testing data and labels.\"\n",
        "    assert X_val.shape[0] == y_val.shape[0], \"Mismatch in validation data and labels.\"\n",
        "\n",
        "    print(\"Training set size:\", X_train.shape, y_train.shape)\n",
        "    print(\"Testing set size:\", X_test.shape, y_test.shape)\n",
        "    print(\"Validation set size:\", X_val.shape, y_val.shape)\n",
        "\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "    joblib.dump(X_train, os.path.join(output_dir, 'X_train.pkl'))\n",
        "    joblib.dump(X_test, os.path.join(output_dir, 'X_test.pkl'))\n",
        "    joblib.dump(X_val, os.path.join(output_dir, 'X_val.pkl'))\n",
        "    joblib.dump(y_train, os.path.join(output_dir, 'y_train.pkl'))\n",
        "    joblib.dump(y_test, os.path.join(output_dir, 'y_test.pkl'))\n",
        "    joblib.dump(y_val, os.path.join(output_dir, 'y_val.pkl'))\n",
        "\n",
        "    if X_train_resampled is not None and y_train_resampled is not None:\n",
        "        joblib.dump(X_train_resampled, os.path.join(output_dir, 'X_train_resampled.pkl'))\n",
        "        joblib.dump(y_train_resampled, os.path.join(output_dir, 'y_train_resampled.pkl'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "q4ympeT2t9yP",
      "metadata": {
        "id": "q4ympeT2t9yP"
      },
      "outputs": [],
      "source": [
        "def fill_missing(X_train, X_test, X_val):\n",
        "  columns_to_fill = ['cancer', 'cigarette_smoking', 'asthma', 'copd', 'coronary_artery_disease']\n",
        "  for column in columns_to_fill:\n",
        "    if column in X_train.columns:\n",
        "      X_train[column], X_test[column] = replace_unknown_with_mode(X_train[column], X_test[column])\n",
        "      X_train[column], X_val[column] = replace_unknown_with_mode(X_train[column], X_val[column])\n",
        "  return X_train, X_test, X_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce9B02nyuPQb",
      "metadata": {
        "id": "ce9B02nyuPQb"
      },
      "outputs": [],
      "source": [
        "def encode_ordinal_columns(X_train, X_test, X_val):\n",
        "  cigarette_ordering = [['Never', 'Former', 'Current']]\n",
        "  respiratory_support_ordering = [['None', 'Other', 'Nasal cannula', 'High flow nasal cannula (HFNC)']]\n",
        "  encoder_cigarette = OrdinalEncoder(categories=cigarette_ordering)\n",
        "  encoder_respiratory_support = OrdinalEncoder(categories=respiratory_support_ordering)\n",
        "  columns_to_encode = ['cigarette_smoking', 'respiratory_support']\n",
        "\n",
        "  for column in columns_to_encode:\n",
        "      if column in X_train.columns:\n",
        "        if column == 'cigarette_smoking':\n",
        "            X_train['cigarette_smoking'] = encoder_cigarette.fit_transform(X_train[[column]])\n",
        "            X_test['cigarette_smoking'] = encoder_cigarette.transform(X_test[[column]])\n",
        "            X_val['cigarette_smoking'] = encoder_cigarette.transform(X_val[[column]])\n",
        "        elif column == 'respiratory_support':\n",
        "            X_train['respiratory_support'] = encoder_respiratory_support.fit_transform(X_train[[column]])\n",
        "            X_test['respiratory_support'] = encoder_respiratory_support.transform(X_test[[column]])\n",
        "            X_val['respiratory_support'] = encoder_respiratory_support.transform(X_val[[column]])\n",
        "  return X_train, X_test, X_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hP8TfDekuhWH",
      "metadata": {
        "id": "hP8TfDekuhWH"
      },
      "outputs": [],
      "source": [
        "def encode_binary_columns(X_train, X_test, X_val):\n",
        "  binary_columns = ['sex', 'kidney_disease', 'chronic_hypertension', 'cancer', 'asthma', 'copd', 'coronary_artery_disease']\n",
        "  binary_mapping = {'Male': 1, 'Yes': 1, 'Female': 0, 'No': 0}\n",
        "\n",
        "  for column in binary_columns:\n",
        "    if column in X_train.columns:\n",
        "      X_train[column] = X_train[column].map(binary_mapping)\n",
        "      X_val[column] = X_val[column].map(binary_mapping)\n",
        "      X_test[column] = X_test[column].map(binary_mapping)\n",
        "  return X_train, X_test, X_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-AYonICfT5Lx",
      "metadata": {
        "id": "-AYonICfT5Lx"
      },
      "outputs": [],
      "source": [
        "def corr_matrix(X_train, output_dir):\n",
        "    corr_matrix = X_train.corr().abs()\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(corr_matrix, cmap='coolwarm', center=0, linewidths=1, annot=False, fmt=\".2f\")\n",
        "\n",
        "    output_file = os.path.join(output_dir, 'correlation_matrix.png')\n",
        "\n",
        "    plt.savefig(output_file)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tEoP67iGUvH5",
      "metadata": {
        "id": "tEoP67iGUvH5"
      },
      "outputs": [],
      "source": [
        "def multicollinearity(X_test, output_dir):\n",
        "    df_const = add_constant(X_test)\n",
        "\n",
        "    vif_data = pd.DataFrame()\n",
        "    vif_data[\"Variable\"] = df_const.columns\n",
        "    vif_data[\"VIF\"] = [variance_inflation_factor(df_const.values, i) for i in range(df_const.shape[1])]\n",
        "\n",
        "    print(vif_data)\n",
        "\n",
        "    output_path = os.path.join(output_dir, \"vif_data.csv\")\n",
        "\n",
        "    vif_data.to_csv(output_path, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SDFEbPM9jh9O",
      "metadata": {
        "id": "SDFEbPM9jh9O"
      },
      "outputs": [],
      "source": [
        "proteomics_scaling_selection = Pipeline(steps=[\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('selector', RFE(estimator=LogisticRegression(), n_features_to_select=100))\n",
        "])\n",
        "\n",
        "metabolomics_scaling_selection = Pipeline(steps=[\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('selector', RFE(estimator=LogisticRegression(), n_features_to_select=100))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ifBnIkrfk7vP",
      "metadata": {
        "id": "ifBnIkrfk7vP"
      },
      "outputs": [],
      "source": [
        "def omics_pipeline(X_train, X_val, X_test, y_train, data_type):\n",
        "    if data_type == 'Clinical':\n",
        "        print(\"Clinical data received. Skipping preprocessing.\")\n",
        "        return X_train, X_val, X_test\n",
        "\n",
        "    elif data_type == 'Metabolomics':\n",
        "        scaling_selection = metabolomics_scaling_selection\n",
        "    elif data_type == 'Proteomics':\n",
        "        scaling_selection = proteomics_scaling_selection\n",
        "    else:\n",
        "        raise ValueError(f\"Data type {data_type} is not supported for this preprocessing.\")\n",
        "\n",
        "    X_train_preprocessed = scaling_selection.fit_transform(X_train, y_train)\n",
        "    selected_features = X_train.columns[scaling_selection.named_steps['selector'].support_]\n",
        "\n",
        "    X_val_preprocessed = scaling_selection.transform(X_val)\n",
        "    X_test_preprocessed = scaling_selection.transform(X_test)\n",
        "\n",
        "    X_train_preprocessed = pd.DataFrame(X_train_preprocessed, columns=selected_features, index=X_train.index)\n",
        "    X_val_preprocessed = pd.DataFrame(X_val_preprocessed, columns=selected_features, index=X_val.index)\n",
        "    X_test_preprocessed = pd.DataFrame(X_test_preprocessed, columns=selected_features, index=X_test.index)\n",
        "\n",
        "    return X_train_preprocessed, X_val_preprocessed, X_test_preprocessed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TKU5kK1Cvrkx",
      "metadata": {
        "id": "TKU5kK1Cvrkx"
      },
      "outputs": [],
      "source": [
        "def pca_process(X_train_preprocessed, X_val_preprocessed, X_test_preprocessed, data_type):\n",
        "  original_columns = X_train_preprocessed.columns.tolist()\n",
        "\n",
        "  pca = PCA(.90)\n",
        "\n",
        "  X_train_preprocessed = pd.DataFrame(pca.fit_transform(X_train_preprocessed), index=X_train_preprocessed.index)\n",
        "  X_test_preprocessed = pd.DataFrame(pca.transform(X_test_preprocessed), index=X_test_preprocessed.index)\n",
        "  X_val_preprocessed = pd.DataFrame(pca.transform(X_val_preprocessed), index=X_val_preprocessed.index)\n",
        "\n",
        "  loadings = pca.components_.T * np.sqrt(pca.explained_variance_)\n",
        "\n",
        "  loading_matrix = pd.DataFrame(loadings, index=original_columns)\n",
        "  loading_matrix\n",
        "\n",
        "  load_trans = loading_matrix.T\n",
        "  loading_pow = {}\n",
        "\n",
        "\n",
        "  for i in load_trans.columns:\n",
        "      magnitude = load_trans[i].pow(2).sum()\n",
        "      loading_pow[i] = magnitude\n",
        "\n",
        "  file_path_matrix = 'loading_matrix_' + data_type + '.xlsx'\n",
        "  loading_matrix.to_excel(file_path_matrix)\n",
        "\n",
        "  file_path_pow = 'loadingspow' + data_type + '.xlsx'\n",
        "  pd.DataFrame.from_dict(loading_pow, orient='index').to_excel(file_path_pow)\n",
        "\n",
        "  return X_train_preprocessed, X_val_preprocessed, X_test_preprocessed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "145a6a7b",
      "metadata": {
        "id": "145a6a7b"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "import xgboost as xgb\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from numpy import mean\n",
        "from sklearn.metrics import make_scorer, accuracy_score, roc_auc_score, f1_score, precision_score, recall_score\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "\n",
        "models = {\n",
        "    'logistic_regression': LogisticRegression(),\n",
        "    'decision_tree': DecisionTreeClassifier(),\n",
        "    'rand_forest': RandomForestClassifier(),\n",
        "    'extra_trees': ExtraTreesClassifier(),\n",
        "    'mlp': MLPClassifier(),\n",
        "    'svm': SVC(),\n",
        "    'ada_boost': AdaBoostClassifier(),\n",
        "    'xgb': xgb.XGBClassifier(),\n",
        "    'gbm': GradientBoostingClassifier(),\n",
        "    'qda': QuadraticDiscriminantAnalysis(),\n",
        "}\n",
        "\n",
        "log_params = {\n",
        "    'logistic_regression__penalty': ['l1', 'l2'],\n",
        "    'logistic_regression__C': [0.1, 1, 10],\n",
        "    'logistic_regression__solver': ['liblinear', 'saga'],\n",
        "    'logistic_regression__max_iter': [100, 200, 300, 400, 500, 600, 700, 800, 900],\n",
        "    'logistic_regression__class_weight': [None, 'balanced']\n",
        "}\n",
        "\n",
        "\n",
        "dt_params = {\n",
        "    'decision_tree__criterion': ['gini', 'entropy'],\n",
        "    'decision_tree__max_depth': [None,1, 5, 10, 50, 100],\n",
        "    'decision_tree__min_samples_split': [2, 5, 10],\n",
        "    'decision_tree__min_samples_leaf': [1, 2, 4],\n",
        "    'decision_tree__splitter':['best','random'],\n",
        "    'decision_tree__class_weight': [None, 'balanced']\n",
        "}\n",
        "\n",
        "rf_params = {\n",
        "    'rand_forest__n_estimators':[50,100,500,1000,2000],\n",
        "    'rand_forest__criterion': ['gini', 'entropy'],\n",
        "    'rand_forest__max_depth': [None, 5, 10, 20],\n",
        "    'rand_forest__min_samples_split': [2, 5, 10],\n",
        "    'rand_forest__min_samples_leaf': [1, 2, 4],\n",
        "    'rand_forest__class_weight': [None, 'balanced']\n",
        "}\n",
        "\n",
        "et_params = {\n",
        "    'extra_trees__n_estimators': [100, 200, 500, 1000, 2000],\n",
        "    'extra_trees__criterion': ['gini', 'entropy'],\n",
        "    'extra_trees__max_depth': [None, 5, 10],\n",
        "    'extra_trees__min_samples_split': [2, 5, 10],\n",
        "    'extra_trees__min_samples_leaf': [1, 2, 4],\n",
        "    'extra_trees__class_weight': [None, 'balanced']\n",
        "}\n",
        "\n",
        "mlp_params = {\n",
        "    'mlp__hidden_layer_sizes': [(100,), (100, 50), (50, 50), (50,50,50)],\n",
        "    'mlp__activation': ['relu', 'tanh', 'logistic', 'identity'],\n",
        "    'mlp__solver': ['adam', 'sgd', 'lbfgs'],\n",
        "    'mlp__learning_rate':['constant','adaptive'],\n",
        "    'mlp__alpha': [0.0001, 0.001, 0.01, 0.05],\n",
        "    'mlp__max_iter': [100, 200, 300, 400, 500, 600, 700, 800, 900]\n",
        "}\n",
        "\n",
        "svm_params = {\n",
        "    'svm__C':[0.1, 1, 2, 5, 10, 50, 100, 500],\n",
        "    'svm__kernel': ['linear', 'rbf', 'poly', 'sigmoid'],\n",
        "    'svm__gamma': ['scale', 'auto'],\n",
        "    'svm__probability': [True],\n",
        "    'svm__class_weight': [None, 'balanced'],\n",
        "    'svm__max_iter': [100, 200, 300, 400, 500, 600, 700, 800, 900]\n",
        "}\n",
        "\n",
        "ada_params = {\n",
        "    'ada_boost__n_estimators': [10, 20, 30, 50, 100, 200, 500, 1000],\n",
        "    'ada_boost__learning_rate': [0.1, 0.5, 1.0, 2.0, 5.0, 10.0]\n",
        "}\n",
        "\n",
        "xgb_params = {\n",
        "    'xgb__n_estimators': [100, 200, 500],\n",
        "    'xgb__learning_rate': [0.01, 0.03, 0.06, 0.1, 0.2, 0.25, 0.3, 0.4, 0.5, 0.6, 0.7],\n",
        "    'xgb__max_depth': [3, 5, 7, 9, 11],\n",
        "    'xgb__subsample': [0.8, 1.0],\n",
        "    'xgb__colsample_bytree': [0.8, 1.0],\n",
        "    'xgb__gamma': [0, 0.1, 0.2, 0.4, 0.8, 1.6, 3.2, 6.4, 12.8, 25.6, 51.2, 102.4, 200],\n",
        "    'xgb__reg_alpha': [0, 0.1, 0.2, 0.4, 0.8, 1.6, 3.2, 6.4, 12.8, 25.6, 51.2, 102.4, 200],\n",
        "    'xgb__reg_lambda': [0, 0.1, 0.2, 0.4, 0.8, 1.6, 3.2, 6.4, 12.8, 25.6, 51.2, 102.4, 200]\n",
        "}\n",
        "\n",
        "gbm_params = {\n",
        "    'gbm__n_estimators': [50, 100, 200, 500, 1000, 2000],\n",
        "    'gbm__learning_rate': [0.01, 0.1, 0.2, 0.3, 0.4],\n",
        "    'gbm__max_depth': [3, 5, 7],\n",
        "    'gbm__subsample': [0.8, 1.0],\n",
        "    'gbm__max_features': ['sqrt', 'log2']\n",
        "}\n",
        "\n",
        "qda_params = {\n",
        "    'qda__reg_param': [0, 0.5, 1]\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "params = {\n",
        "    'logistic_regression': log_params,\n",
        "    'decision_tree': dt_params,\n",
        "    'rand_forest': rf_params,\n",
        "    'extra_trees': et_params,\n",
        "    'mlp': mlp_params,\n",
        "    'svm': svm_params,\n",
        "    'ada_boost': ada_params,\n",
        "    'xgb': xgb_params,\n",
        "    'gbm': gbm_params,\n",
        "    'qda': qda_params,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec9d27a8",
      "metadata": {
        "id": "ec9d27a8"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from imblearn.pipeline import Pipeline\n",
        "from imblearn.over_sampling import BorderlineSMOTE, SVMSMOTE, RandomOverSampler\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.metrics import classification_report\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def model_search(models, params, over, under, X_train, y_train, X_test, y_test, X_val, y_val):\n",
        "    print(f\"Number of models: {len(models)}\")\n",
        "    training_times = []\n",
        "    max_score = 0\n",
        "    max_model = None\n",
        "    max_model_params = None\n",
        "    estimators_modelsearch = pd.DataFrame()\n",
        "    models_est = []\n",
        "    parameters = []\n",
        "    lscore = []\n",
        "    lroc = []\n",
        "    lfscore_macro = []\n",
        "    lfscore_weighted = []\n",
        "    lprecision = []\n",
        "    lrecall = []\n",
        "    lrecall_severe = []\n",
        "    lclass_report = []\n",
        "    lcvscore = []\n",
        "    lscore_val = []\n",
        "    lroc_val = []\n",
        "    lfscore_macro_val = []\n",
        "    lfscore_weighted_val = []\n",
        "    lprecision_val = []\n",
        "    lrecall_val = []\n",
        "    lrecall_severe_val = []\n",
        "    lclass_report_val = []\n",
        "\n",
        "    scoring = {\n",
        "        'balanced_accuracy': 'balanced_accuracy',\n",
        "        'accuracy': 'accuracy',\n",
        "        'roc_auc': 'roc_auc_ovr',\n",
        "        'precision': 'precision_macro',\n",
        "        'recall_macro': make_scorer(recall_score, average='macro'),\n",
        "        'f1_macro': make_scorer(f1_score, average='macro'),\n",
        "        'f1_weighted': make_scorer(f1_score, average='weighted')\n",
        "    }\n",
        "\n",
        "    cv = StratifiedKFold(10)\n",
        "\n",
        "\n",
        "    for i, j in models.items():\n",
        "        start_time = time.time()\n",
        "\n",
        "        try:\n",
        "            pipeline = Pipeline([('over', over), ('under', under), (i, j)])\n",
        "\n",
        "            rs = RandomizedSearchCV(\n",
        "                estimator=pipeline,\n",
        "                param_distributions=params[i],\n",
        "                scoring=scoring,\n",
        "                refit='roc_auc',\n",
        "                cv=cv,\n",
        "                n_iter=10,\n",
        "                random_state=42,\n",
        "                n_jobs=-1\n",
        "            )\n",
        "\n",
        "            rs.fit(X_train, y_train)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error while fitting model {i}: {e}\")\n",
        "            continue\n",
        "\n",
        "\n",
        "\n",
        "        y_pred = rs.predict(X_test)\n",
        "        class_report = classification_report(y_test, y_pred)\n",
        "        print(f\"Classification Report for {i}:\\n\", class_report)\n",
        "\n",
        "        y_pred_val = rs.predict(X_val)\n",
        "        class_report_val = classification_report(y_val, y_pred_val)\n",
        "        print(f\"Validation Classification Report for {i}:\\n\", class_report_val)\n",
        "\n",
        "        try:\n",
        "            y_pred_proba = rs.predict_proba(X_test)\n",
        "            lroc.append(roc_auc_score(y_test, y_pred_proba, multi_class='ovr'))\n",
        "        except AttributeError:\n",
        "            print(f\"{i} does not support probability prediction. ROC AUC score cannot be computed.\")\n",
        "            lroc.append(None)\n",
        "\n",
        "        try:\n",
        "            y_pred_proba_val = rs.predict_proba(X_val)\n",
        "            lroc_val.append(roc_auc_score(y_val, y_pred_proba_val, multi_class='ovr'))\n",
        "        except AttributeError:\n",
        "            print(f\"{i} does not support probability prediction. ROC AUC score cannot be computed for validation set.\")\n",
        "            lroc_val.append(None)\n",
        "\n",
        "\n",
        "\n",
        "        lscore.append(accuracy_score(y_test, y_pred))\n",
        "        lscore_val.append(accuracy_score(y_val, y_pred_val))\n",
        "        lcvscore.append(rs.best_score_)\n",
        "\n",
        "        from sklearn.exceptions import UndefinedMetricWarning\n",
        "        import warnings\n",
        "\n",
        "        try:\n",
        "            lprecision.append(precision_score(y_test, y_pred, average='macro', zero_division=0))\n",
        "            lprecision_val.append(precision_score(y_val, y_pred_val, average='macro', zero_division=0))\n",
        "        except UndefinedMetricWarning:\n",
        "            print(f\"Warning encountered for model {i}\")\n",
        "            print(\"Test predictions:\", y_pred)\n",
        "            print(\"Validation predictions:\", y_pred_val)\n",
        "\n",
        "\n",
        "        lrecall.append(recall_score(y_test, y_pred, average='macro'))\n",
        "        lrecall_val.append(recall_score(y_val, y_pred_val, average='macro'))\n",
        "\n",
        "        lfscore_macro.append(f1_score(y_test, y_pred, average='macro'))\n",
        "        lfscore_macro_val.append(f1_score(y_val, y_pred_val, average='macro'))\n",
        "\n",
        "        lfscore_weighted.append(f1_score(y_test, y_pred, average='weighted'))\n",
        "        lfscore_weighted_val.append(f1_score(y_val, y_pred_val, average='weighted'))\n",
        "\n",
        "        lrecall_severe.append(recall_score(y_test, y_pred, labels=[2], average='macro'))\n",
        "        lrecall_severe_val.append(recall_score(y_val, y_pred_val, labels=[2], average='macro'))\n",
        "\n",
        "        lclass_report.append(class_report)\n",
        "        lclass_report_val.append(class_report_val)\n",
        "\n",
        "        if lscore[-1] > max_score:\n",
        "            max_score = lscore[-1]\n",
        "            max_model = rs.best_estimator_\n",
        "            max_model_params = rs.best_params_\n",
        "\n",
        "        models_est.append(i)\n",
        "        parameters.append(rs.best_params_)\n",
        "\n",
        "    estimators_modelsearch['Models'] = models_est\n",
        "    estimators_modelsearch['Best parameters'] = parameters\n",
        "    estimators_modelsearch['ROC AUC'] = lroc\n",
        "    estimators_modelsearch['ROC AUC Validation'] = lroc_val\n",
        "    estimators_modelsearch['F1-score (macro)'] = lfscore_macro\n",
        "    estimators_modelsearch['F1-score (macro) Validation'] = lfscore_macro_val\n",
        "    estimators_modelsearch['F1-score (weighted)'] = lfscore_weighted\n",
        "    estimators_modelsearch['F1-score (weighted) Validation'] = lfscore_weighted_val\n",
        "    estimators_modelsearch['Precision'] = lprecision\n",
        "    estimators_modelsearch['Precision Validation'] = lprecision_val\n",
        "    estimators_modelsearch['Recall'] = lrecall\n",
        "    estimators_modelsearch['Recall Severe'] = lrecall_severe\n",
        "    estimators_modelsearch['Recall Severe Validation'] = lrecall_severe_val\n",
        "    estimators_modelsearch['Score'] = lscore\n",
        "    estimators_modelsearch['Score Validation'] = lscore_val\n",
        "    estimators_modelsearch['CV Score'] = lcvscore\n",
        "    print(len(estimators_modelsearch))\n",
        "\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.bar(models_est, lscore, label='Test Score')\n",
        "    plt.bar(models_est, lscore_val, label='Validation Score', alpha=0.5)\n",
        "    plt.legend()\n",
        "    plt.title(\"Performance Comparison\")\n",
        "    plt.show()\n",
        "\n",
        "    return [max_score, max_model, max_model_params], estimators_modelsearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OlW058Dgou89",
      "metadata": {
        "id": "OlW058Dgou89"
      },
      "outputs": [],
      "source": [
        "def main(data_paths):\n",
        "  y = None\n",
        "  for data_path in data_paths:\n",
        "    data, data_type = get_data_type(data_path, file_type='excel')\n",
        "    if data_type == 'Clinical':\n",
        "        output_dir = os.path.join('covid_baseline_classifier_outputs', data_type)\n",
        "        X, y = preprocess_clinical(data, output_dir)\n",
        "    elif data_type == 'Metabolomics':\n",
        "        if y is None:\n",
        "          raise ValueError(\"Clinical data must be processed first to set 'y'.\")\n",
        "        output_dir = os.path.join('covid_baseline_classifier_outputs', data_type)\n",
        "        X = preprocess_metabolomics(data, output_dir)\n",
        "    elif data_type == 'Proteomics':\n",
        "        if y is None:\n",
        "          raise ValueError(\"Clinical data must be processed first to set 'y'.\")\n",
        "        output_dir = os.path.join('covid_baseline_classifier_outputs', data_type)\n",
        "        X = preprocess_proteomics(data, output_dir)\n",
        "\n",
        "\n",
        "\n",
        "    print(np.unique(y))\n",
        "    print(\"X shape:\", X.shape)\n",
        "    print(\"y shape:\", y.shape)\n",
        "    print(\"Size before first split:\", X.shape, y.shape)\n",
        "    X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "    print(\"Size after first split - Test set:\", X_test.shape, y_test.shape)\n",
        "\n",
        "    print(\"Size before second split:\", X_temp.shape, y_temp.shape)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
        "    print(\"Sizes after second split - Train and Val set:\", X_train.shape, y_train.shape, X_val.shape, y_val.shape)\n",
        "\n",
        "    print(f\"{data_type} Train Target:\", y_train.value_counts(normalize=True))\n",
        "\n",
        "    X_train = X_train.copy()\n",
        "    X_test = X_test.copy()\n",
        "    X_val = X_val.copy()\n",
        "\n",
        "    X_train, X_test, X_val = fill_missing(X_train, X_test, X_val)\n",
        "    X_train, X_test, X_val = encode_ordinal_columns(X_train, X_test, X_val)\n",
        "\n",
        "\n",
        "    if 'age' in X_train.columns:\n",
        "      age_train = X_train.pop('age')\n",
        "      X_train['age'] = age_train\n",
        "      X_train.head(5)\n",
        "      age_val = X_val.pop('age')\n",
        "      X_val['age'] = age_val\n",
        "      age_test = X_test.pop('age')\n",
        "      X_test['age'] = age_test\n",
        "\n",
        "    #for data_path in data_paths:\n",
        "    if data_type == 'Clinical':\n",
        "        X_train, X_test, X_val = encode_binary_columns(X_train, X_test, X_val)\n",
        "        corr_matrix(X_train, output_dir)\n",
        "        multicollinearity(X_test, output_dir)\n",
        "\n",
        "        print(\"Class distribution before resampling:\", np.bincount(y_train))\n",
        "        over = BorderlineSMOTE(sampling_strategy='auto')\n",
        "        under = RandomUnderSampler(sampling_strategy='majority')\n",
        "\n",
        "        pipeline = Pipeline([\n",
        "            ('over', over),\n",
        "            ('under', under)\n",
        "        ])\n",
        "\n",
        "        X_train_resampled, y_train_resampled = pipeline.fit_resample(X_train, y_train)\n",
        "\n",
        "        print(\"Class distribution after resampling:\", np.bincount(y_train_resampled))\n",
        "\n",
        "        results, df = model_search(\n",
        "          models=models,\n",
        "          params=params,\n",
        "          over=over,\n",
        "          under=under,\n",
        "          X_train=X_train,\n",
        "          y_train=y_train,\n",
        "          X_test=X_test,\n",
        "          y_test=y_test,\n",
        "          X_val=X_val,\n",
        "          y_val=y_val\n",
        "        )\n",
        "        save_data_splits(X_train, X_test, X_val, y_train, y_test, y_val, os.path.join('covid_baseline_classifier_outputs', data_type), X_train_resampled, y_train_resampled)\n",
        "        file_path = data_type + '_model_training_results.xlsx'\n",
        "        df.to_excel(file_path)\n",
        "\n",
        "    elif data_type == 'Metabolomics':\n",
        "        X_train_preprocessed, X_val_preprocessed, X_test_preprocessed = omics_pipeline(X_train, X_val, X_test, y_train, data_type)\n",
        "        X_train_preprocessed, X_val_preprocessed, X_test_preprocessed = pca_process(X_train_preprocessed, X_val_preprocessed, X_test_preprocessed, data_type)\n",
        "        print(\"Class distribution before resampling:\", np.bincount(y_train))\n",
        "        over = BorderlineSMOTE(sampling_strategy='auto')\n",
        "        under = RandomUnderSampler(sampling_strategy='majority')\n",
        "\n",
        "        pipeline = Pipeline([\n",
        "            ('over', over),\n",
        "            ('under', under)\n",
        "        ])\n",
        "\n",
        "        X_train_resampled, y_train_resampled = pipeline.fit_resample(X_train_preprocessed, y_train)\n",
        "\n",
        "        print(\"Class distribution after resampling:\", np.bincount(y_train_resampled))\n",
        "\n",
        "        results, df = model_search(\n",
        "          models=models,\n",
        "          params=params,\n",
        "          over=over,\n",
        "          under=under,\n",
        "          X_train=X_train_preprocessed,\n",
        "          y_train=y_train,\n",
        "          X_test=X_test_preprocessed,\n",
        "          y_test=y_test,\n",
        "          X_val=X_val_preprocessed,\n",
        "          y_val=y_val\n",
        "        )\n",
        "\n",
        "        file_path = data_type + '_model_training_results.xlsx'\n",
        "        save_data_splits(X_train_resampled, X_test_preprocessed, X_val_preprocessed, y_train_resampled, y_test, y_val, os.path.join('covid_baseline_classifier_outputs', data_type))\n",
        "        df.to_excel(file_path)\n",
        "\n",
        "    elif data_type == 'Proteomics':\n",
        "        X_train_preprocessed, X_val_preprocessed, X_test_preprocessed = omics_pipeline(X_train, X_val, X_test, y_train, data_type)\n",
        "        X_train_preprocessed, X_val_preprocessed, X_test_preprocessed = pca_process(X_train_preprocessed, X_val_preprocessed, X_test_preprocessed, data_type)\n",
        "        print(\"Class distribution before resampling:\", np.bincount(y_train))\n",
        "        over = BorderlineSMOTE(sampling_strategy='auto')\n",
        "        under = RandomUnderSampler(sampling_strategy='majority')\n",
        "\n",
        "        pipeline = Pipeline([\n",
        "            ('over', over),\n",
        "            ('under', under)\n",
        "        ])\n",
        "\n",
        "        X_train_resampled, y_train_resampled = pipeline.fit_resample(X_train_preprocessed, y_train)\n",
        "\n",
        "        print(\"Class distribution after resampling:\", np.bincount(y_train_resampled))\n",
        "\n",
        "        results, df = model_search(\n",
        "          models=models,\n",
        "          params=params,\n",
        "          over=over,\n",
        "          under=under,\n",
        "          X_train=X_train_preprocessed,\n",
        "          y_train=y_train,\n",
        "          X_test=X_test_preprocessed,\n",
        "          y_test=y_test,\n",
        "          X_val=X_val_preprocessed,\n",
        "          y_val=y_val\n",
        "        )\n",
        "\n",
        "        file_path = data_type + '_model_training_results.xlsx'\n",
        "        save_data_splits(X_train_resampled, X_test_preprocessed, X_val_preprocessed, y_train_resampled, y_test, y_val, os.path.join('covid_baseline_classifier_outputs', data_type))\n",
        "        df.to_excel(file_path)\n",
        "\n",
        "\n",
        "  return X_train_preprocessed, X_val_preprocessed, X_test_preprocessed, y_train, y_test, y_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Q0Pv1bRR9mfR",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Q0Pv1bRR9mfR",
        "outputId": "103c65df-966a-4f4c-b63f-2edce88e1432"
      },
      "outputs": [],
      "source": [
        "data_paths = [\"/content/drive/MyDrive/Master's Thesis/COVID-19/final-project/final_data/ISB/isb_clinical.xlsx\",\n",
        "              \"/content/drive/MyDrive/Master's Thesis/COVID-19/final-project/final_data/ISB/isb_metabolomics.xlsx\",\n",
        "              \"/content/drive/MyDrive/Master's Thesis/COVID-19/final-project/final_data/ISB/isb_proteomics.xlsx\"\n",
        "              ]\n",
        "main(data_paths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jLt9aJ0jIx2k",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLt9aJ0jIx2k",
        "outputId": "e31a334b-c4ed-48d1-8f47-f4b09ea5ebc4"
      },
      "outputs": [],
      "source": [
        "!zip -r covid_baseline_classifier_outputs.zip covid_baseline_classifier_outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ba158d1",
      "metadata": {
        "id": "8ba158d1",
        "scrolled": true
      },
      "source": [
        "## Interpretability on Individual Models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wFsWQDXsYXG4",
      "metadata": {
        "id": "wFsWQDXsYXG4"
      },
      "source": [
        "{'xgb__subsample': 1.0, 'xgb__reg_lambda': 0.2, 'xgb__reg_alpha': 0.8, 'xgb__n_estimators': 200, 'xgb__max_depth': 3, 'xgb__learning_rate': 0.25, 'xgb__gamma': 0.2, 'xgb__colsample_bytree': 0.8}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74d6b0fc",
      "metadata": {
        "id": "74d6b0fc"
      },
      "outputs": [],
      "source": [
        "proteomics_model = xgb.XGBClassifier(\n",
        "        subsample = 1.0,\n",
        "        reg_lambda = 0.2,\n",
        "        reg_alpha = 0.8,\n",
        "        n_estimators = 200,\n",
        "        max_depth = 3,\n",
        "        learning_rate = 0.25,\n",
        "        gamma = 0.2,\n",
        "        colsample_bytree = 0.8\n",
        ")\n",
        "\n",
        "metabolomics_model = SVC(\n",
        "        probability = True,\n",
        "        max_iter = 700,\n",
        "        kernel = 'rbf',\n",
        "        gamma = 'scale',\n",
        "        class_weight = 'balanced',\n",
        "        C = 500,\n",
        ")\n",
        "\n",
        "clinical_model = RandomForestClassifier(\n",
        "    n_estimators = 50,\n",
        "    min_samples_split = 10,\n",
        "    min_samples_leaf =  2,\n",
        "    max_depth = 20,\n",
        "    criterion = 'entropy',\n",
        "    class_weight = None\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AxkCBploVE3n",
      "metadata": {
        "id": "AxkCBploVE3n"
      },
      "outputs": [],
      "source": [
        "def load_pre_split_data():\n",
        "        X_train_pro = joblib.load('/content/drive/MyDrive/covid_baseline_classifier_outputs/Proteomics/X_train.pkl')\n",
        "        y_train_pro = joblib.load('/content/drive/MyDrive/covid_baseline_classifier_outputs/Proteomics/y_train.pkl')\n",
        "        X_test_pro = joblib.load('/content/drive/MyDrive/covid_baseline_classifier_outputs/Proteomics/X_test.pkl')\n",
        "        y_test_pro = joblib.load('/content/drive/MyDrive/covid_baseline_classifier_outputs/Proteomics/y_test.pkl')\n",
        "        X_val_pro = joblib.load('/content/drive/MyDrive/covid_baseline_classifier_outputs/Proteomics/X_val.pkl')\n",
        "        y_val_pro = joblib.load('/content/drive/MyDrive/covid_baseline_classifier_outputs/Proteomics/y_val.pkl')\n",
        "\n",
        "        X_train_met = joblib.load('/content/drive/MyDrive/covid_baseline_classifier_outputs/Metabolomics/X_train.pkl')\n",
        "        y_train_met = joblib.load('/content/drive/MyDrive/covid_baseline_classifier_outputs/Metabolomics/y_train.pkl')\n",
        "        X_test_met = joblib.load('/content/drive/MyDrive/covid_baseline_classifier_outputs/Metabolomics/X_test.pkl')\n",
        "        y_test_met = joblib.load('/content/drive/MyDrive/covid_baseline_classifier_outputs/Metabolomics/y_test.pkl')\n",
        "        X_val_met = joblib.load('/content/drive/MyDrive/covid_baseline_classifier_outputs/Metabolomics/X_val.pkl')\n",
        "        y_val_met = joblib.load('/content/drive/MyDrive/covid_baseline_classifier_outputs/Metabolomics/y_val.pkl')\n",
        "\n",
        "        X_train_clin = joblib.load('/content/drive/MyDrive/covid_baseline_classifier_outputs/Clinical/X_train.pkl')\n",
        "        y_train_clin = joblib.load('/content/drive/MyDrive/covid_baseline_classifier_outputs/Clinical/y_train.pkl')\n",
        "        X_test_clin = joblib.load('/content/drive/MyDrive/covid_baseline_classifier_outputs/Clinical/X_test.pkl')\n",
        "        y_test_clin = joblib.load('/content/drive/MyDrive/covid_baseline_classifier_outputs/Clinical/y_test.pkl')\n",
        "        X_val_clin = joblib.load('/content/drive/MyDrive/covid_baseline_classifier_outputs/Clinical/X_val.pkl')\n",
        "        y_val_clin = joblib.load('/content/drive/MyDrive/covid_baseline_classifier_outputs/Clinical/y_val.pkl')\n",
        "\n",
        "        return X_train_met, y_train_met, X_test_met, y_test_met, X_val_met, y_val_met, X_train_pro, y_train_pro, X_test_pro, y_test_pro, X_val_pro, y_val_pro, X_train_clin, y_train_clin, X_test_clin, y_test_clin, X_val_clin, y_val_clin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SZ2uStaGid8O",
      "metadata": {
        "id": "SZ2uStaGid8O"
      },
      "outputs": [],
      "source": [
        "def load_model(data_type):\n",
        "  if data_type == \"Proteomics\":\n",
        "      model = proteomics_model\n",
        "\n",
        "      explainer_type = 'tree'\n",
        "      shap_filename = 'pro_shap.svg'\n",
        "      lime_filename = 'pro_lime.html'\n",
        "      cm_filename = 'pro_confusion_matrix.png'\n",
        "      output_file_name = 'pro_feature_importance.xlsx'\n",
        "      index = 10\n",
        "  elif data_type == \"Metabolomics\":\n",
        "      model = metabolomics_model\n",
        "\n",
        "      explainer_type = 'kernel'\n",
        "      shap_filename = 'met_shap.svg'\n",
        "      lime_filename = 'met_lime.html'\n",
        "      cm_filename = 'met_confusion_matrix.png'\n",
        "      output_file_name = 'met_feature_importance.xlsx'\n",
        "      index = 10\n",
        "  elif data_type == \"Clinical\":\n",
        "      model = clinical_model\n",
        "      loading_matrix = None\n",
        "      explainer_type = 'tree'\n",
        "      shap_filename = 'clin_shap.svg'\n",
        "      lime_filename = 'clin_lime.html'\n",
        "      cm_filename = 'clin_confusion_matrix.png'\n",
        "      output_file_name = 'clin_feature_importance.xlsx'\n",
        "      index = 9\n",
        "  else:\n",
        "      raise ValueError(f\"Unexpected data_type: {data_type}\")\n",
        "  return model, explainer_type, shap_filename, lime_filename, cm_filename, output_file_name, index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a668a15",
      "metadata": {
        "id": "1a668a15"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shap\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from lime import lime_tabular\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "def train_and_explain_model(model, X_train, y_train, X_test, y_test, loading_matrix, explainer_type, shap_filename, lime_filename, cm_filename, output_dir, output_filename, i):\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    joblib.dump(model, os.path.join(output_dir, 'trained_model.pkl'))\n",
        "\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    sns.heatmap(cm, annot=True, fmt='g', cmap='Blues')\n",
        "    plt.xlabel('Predicted labels')\n",
        "    plt.ylabel('True labels')\n",
        "    plt.title('Confusion Matrix')\n",
        "\n",
        "    plt.savefig(cm_filename, dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "    if explainer_type == \"tree\":\n",
        "        explainer = shap.TreeExplainer(model)\n",
        "    elif explainer_type == \"kernel\":\n",
        "        explainer = shap.KernelExplainer(model.predict_proba, X_train)\n",
        "    elif explainer_type == \"linear\":\n",
        "        explainer = shap.LinearExplainer(model, X_train)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid explainer type\")\n",
        "\n",
        "    shap_values = explainer.shap_values(X_train)\n",
        "    shap.summary_plot(shap_values, X_train, feature_names=X_train.columns, max_display=20, show=False)\n",
        "    plt.savefig(shap_filename)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "    if isinstance(shap_values, list):\n",
        "        vals = np.abs(shap_values[0]).mean(0)\n",
        "    else:\n",
        "        vals = np.abs(shap_values).mean(0)\n",
        "\n",
        "    feature_importance = pd.DataFrame(list(zip(X_train.columns, vals)), columns=['col_name', 'feature_importance_vals'])\n",
        "    feature_importance.sort_values(by=['feature_importance_vals'], ascending=False, inplace=True)\n",
        "    feature_importance.to_excel(output_filename)\n",
        "\n",
        "    # loadings\n",
        "    if loading_matrix is not None:\n",
        "        assert isinstance(loading_matrix, pd.DataFrame), \"loading_matrix should be a pandas DataFrame.\"\n",
        "        top_features = [i for i in list(feature_importance['col_name'])[:30] if str(i).isdigit()]\n",
        "        print(top_features)\n",
        "        loading_trans = loading_matrix[top_features].T\n",
        "        loading_pow = {}\n",
        "        for column in loading_trans.columns:\n",
        "            magnitude = loading_trans[column].pow(2).sum()\n",
        "            loading_pow[column] = magnitude\n",
        "        pd.DataFrame.from_dict(loading_pow, orient='index').to_excel(output_filename.replace('.xlsx', '_loading_pow.xlsx'))\n",
        "\n",
        "    explainer = lime_tabular.LimeTabularExplainer(X_train.to_numpy(), feature_names=X_train.columns, class_names=[\"Mild\", \"Moderate\", \"Severe\"], mode='classification', random_state=42)\n",
        "    exp = explainer.explain_instance(X_test.iloc[i].to_numpy(), model.predict_proba, num_features=10)\n",
        "    exp.save_to_file(os.path.join(os.getcwd(), lime_filename))\n",
        "\n",
        "    observation = X_test.iloc[i]\n",
        "    observation_df = pd.DataFrame(observation).transpose()\n",
        "    observation_df['who_severity'] = y_test.iloc[i]\n",
        "    print(observation_df)\n",
        "\n",
        "    y_pred_proba = model.predict_proba(X_test)\n",
        "    return y_pred_proba\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "npEZB4R3VeEg",
      "metadata": {
        "id": "npEZB4R3VeEg"
      },
      "outputs": [],
      "source": [
        "data_paths = ['baseline_classifier_outputs/Clinical',\n",
        "              'baseline_classifier_outputs/Metabolomics',\n",
        "              'baseline_classifier_outputs/Proteomics']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "A_jGeYQlVonY",
      "metadata": {
        "id": "A_jGeYQlVonY"
      },
      "outputs": [],
      "source": [
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "X_train_met, y_train_met, X_test_met, y_test_met, X_val_met, y_val_met, X_train_pro, y_train_pro, X_test_pro, y_test_pro, X_val_pro, y_val_pro, X_train_clin, y_train_clin, X_test_clin, y_test_clin, X_val_clin, y_val_clin = load_pre_split_data()\n",
        "X_train_met.columns = X_train_met.columns.str.replace('[', '').str.replace(']', '').str.replace('<', '')\n",
        "X_test_met.columns = X_test_met.columns.str.replace('[', '').str.replace(']', '').str.replace('<', '')\n",
        "X_train_met.columns = X_train_met.columns.astype(str)\n",
        "X_test_met.columns = X_test_met.columns.astype(str)\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "output_dir_met = \"output/metabolomics\"\n",
        "output_dir_pro = \"output/proteomics\"\n",
        "\n",
        "met_pred_proba_test = train_and_explain_model(\n",
        "  metabolomics_model, X_train_met, y_train_met, X_test_met, y_test_met,\n",
        "  'kernel', 'met_shap.svg', 'met_lime.html', 'met_confusion_matrix.png',\n",
        "  output_dir_met, 'met_feature_importance.xlsx', 10\n",
        ")\n",
        "\n",
        "pro_pred_proba_test = train_and_explain_model(\n",
        "  proteomics_model, X_train_pro, y_train_pro, X_test_pro, y_test_pro,\n",
        "  'tree', 'pro_shap.svg', 'pro_lime.html', 'pro_confusion_matrix.png',\n",
        "  output_dir_pro, 'pro_feature_importance.xlsx', 10\n",
        ")\n",
        "\n",
        "clin_pred_proba_test = train_and_explain_model(\n",
        "  metabolomics_model, X_train_met, y_train_met, X_test_met, y_test_met,\n",
        "  'kernel', 'met_shap.svg', 'met_lime.html', 'met_confusion_matrix.png',\n",
        "  output_dir_met, 'met_feature_importance.xlsx', 10\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ufhtuw039rnm",
      "metadata": {
        "id": "ufhtuw039rnm"
      },
      "outputs": [],
      "source": [
        "shap_values = np.load(\"/content/output/metabolomics/met_feature_importance.xlsx_shap_values.npy\")\n",
        "\n",
        "feature_names = X_train_met.columns\n",
        "\n",
        "print(\"SHAP values shape:\", shap_values.shape)\n",
        "\n",
        "# loop over all classes (3 classes in this case)\n",
        "for class_idx in range(shap_values.shape[2]):\n",
        "    # mean absolute SHAP values for the current class\n",
        "    class_shap_values = np.abs(shap_values[:, :, class_idx]).mean(axis=0)\n",
        "\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'feature': feature_names,\n",
        "        'importance': class_shap_values\n",
        "    })\n",
        "\n",
        "    # sort by importance\n",
        "    feature_importance = feature_importance.sort_values(by='importance', ascending=False)\n",
        "\n",
        "    excel_filename = f\"met_feature_importance_class_{class_idx}.xlsx\"\n",
        "    feature_importance.to_excel(excel_filename, index=False)\n",
        "\n",
        "    print(f\"Feature Importance for Class {class_idx} saved to {excel_filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rnNzCLhs9ugq",
      "metadata": {
        "id": "rnNzCLhs9ugq"
      },
      "outputs": [],
      "source": [
        "shap_values = np.load(\"/content/output/proteomics/pro_feature_importance.xlsx_shap_values.npy\")\n",
        "\n",
        "feature_names = X_train_pro.columns\n",
        "\n",
        "print(\"SHAP values shape:\", shap_values.shape)\n",
        "\n",
        "# loop over all classes (3 classes in this case)\n",
        "for class_idx in range(shap_values.shape[2]):\n",
        "    # mean absolute SHAP values for the current class\n",
        "    class_shap_values = np.abs(shap_values[:, :, class_idx]).mean(axis=0)\n",
        "\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'feature': feature_names,\n",
        "        'importance': class_shap_values\n",
        "    })\n",
        "\n",
        "    # sort by importance\n",
        "    feature_importance = feature_importance.sort_values(by='importance', ascending=False)\n",
        "\n",
        "    excel_filename = f\"pro_feature_importance_class_{class_idx}.xlsx\"\n",
        "    feature_importance.to_excel(excel_filename, index=False)\n",
        "\n",
        "    print(f\"Feature Importance for Class {class_idx} saved to {excel_filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lxFojQJ8-Cd6",
      "metadata": {
        "id": "lxFojQJ8-Cd6"
      },
      "outputs": [],
      "source": [
        "shap_values = np.load(\"/content/output/clinical/clin_feature_importance.xlsx_shap_values.npy\")\n",
        "\n",
        "feature_names = X_train_clin.columns\n",
        "\n",
        "print(\"SHAP values shape:\", shap_values.shape)\n",
        "\n",
        "# loop over all classes (3 classes in this case)\n",
        "for class_idx in range(shap_values.shape[2]):\n",
        "    # mean absolute SHAP values for the current class\n",
        "    class_shap_values = np.abs(shap_values[:, :, class_idx]).mean(axis=0)\n",
        "\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'feature': feature_names,\n",
        "        'importance': class_shap_values\n",
        "    })\n",
        "\n",
        "    # sort by importance\n",
        "    feature_importance = feature_importance.sort_values(by='importance', ascending=False)\n",
        "\n",
        "    excel_filename = f\"clin_feature_importance_class_{class_idx}.xlsx\"\n",
        "    feature_importance.to_excel(excel_filename, index=False)\n",
        "\n",
        "    print(f\"Feature Importance for Class {class_idx} saved to {excel_filename}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90b3d3b1",
      "metadata": {
        "id": "90b3d3b1"
      },
      "source": [
        "## Stacking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a06070e5",
      "metadata": {
        "id": "a06070e5"
      },
      "outputs": [],
      "source": [
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "\n",
        "class CustomModel(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, model, feature_indices):\n",
        "        self.model = model\n",
        "        self.feature_indices = feature_indices\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        self.model.fit(X[:, self.feature_indices], y)\n",
        "        self.classes_ = self.model.classes_  # set the classes_ attribute\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        return self.model.predict_proba(X[:, self.feature_indices])\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.model.predict(X[:, self.feature_indices])\n",
        "\n",
        "    def get_params(self, deep=True):\n",
        "        return {\"model\": self.model, \"feature_indices\": self.feature_indices}\n",
        "\n",
        "    def set_params(self, **parameters):\n",
        "        for parameter, value in parameters.items():\n",
        "            setattr(self, parameter, value)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "B0iHuBd59Z6z",
      "metadata": {
        "id": "B0iHuBd59Z6z"
      },
      "outputs": [],
      "source": [
        "X_train = np.concatenate([X_train_met, X_train_pro, X_train_clin], axis=1)\n",
        "X_test = np.concatenate([X_test_met, X_test_pro, X_test_clin], axis=1)\n",
        "X_val = np.concatenate([X_val_met, X_val_pro, X_val_clin], axis=1)\n",
        "y_train = y_train_clin\n",
        "y_test = y_test_clin\n",
        "y_val = y_val_clin\n",
        "\n",
        "# feature indices for each modality (Metabolomics and Proteomics)\n",
        "\n",
        "metabolomics_feature_indices = list(range(X_train_met.shape[1]))\n",
        "proteomics_feature_indices = list(range(X_train_met.shape[1], X_train_met.shape[1] + X_train_pro.shape[1]))\n",
        "clinical_feature_indices = list(range(X_train_met.shape[1] + X_train_pro.shape[1], X_train_clin.shape[1]))\n",
        "\n",
        "print(\"Metabolomics Feature Indices:\", metabolomics_feature_indices)\n",
        "print(\"Proteomics Feature Indices:\", proteomics_feature_indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "369ec519",
      "metadata": {
        "id": "369ec519",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "base_models = [\n",
        "    ('svc', CustomModel(metabolomics_model, metabolomics_feature_indices)),\n",
        "    ('xgb', CustomModel(proteomics_model, proteomics_feature_indices)),\n",
        "    ('rf', CustomModel(clinical_model, clinical_feature_indices)),\n",
        "]\n",
        "\n",
        "stacked_models = StackingClassifier(estimators=base_models, final_estimator=LogisticRegression(multi_class='multinomial', penalty = 'l2', random_state=42))\n",
        "\n",
        "print(f\"Starting model training at: {time.ctime()}\")\n",
        "start_time = time.time()\n",
        "\n",
        "stacked_models.fit(X_train, y_train)\n",
        "\n",
        "y_pred = stacked_models.predict(X_test)\n",
        "y_pred_proba = stacked_models.predict_proba(X_test)\n",
        "y_pred_val = stacked_models.predict(X_val)\n",
        "y_pred_proba_val = stacked_models.predict_proba(X_val)\n",
        "\n",
        "score = accuracy_score(y_test, y_pred)\n",
        "score_val = accuracy_score(y_val, y_pred_val)\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba, multi_class='ovr')\n",
        "roc_auc_val = roc_auc_score(y_val, y_pred_proba_val, multi_class='ovr')\n",
        "fscore_macro = f1_score(y_test, y_pred, average='macro')\n",
        "fscore_macro_val = f1_score(y_val, y_pred_val, average='macro')\n",
        "fscore_weighted = f1_score(y_test, y_pred, average='weighted')\n",
        "fscore_weighted_val = f1_score(y_val, y_pred_val, average='weighted')\n",
        "precision = precision_score(y_test, y_pred, average='macro')\n",
        "precision_val = precision_score(y_val, y_pred_val, average='macro')\n",
        "recall = recall_score(y_test, y_pred, average='macro')\n",
        "recall_val = recall_score(y_val, y_pred_val, average='macro')\n",
        "recall_severe = recall_score(y_test, y_pred, labels=[2], average='macro')\n",
        "recall_severe_val = recall_score(y_val, y_pred_val, labels=[2], average='macro')\n",
        "\n",
        "\n",
        "end_time = time.time()\n",
        "processing_time = end_time - start_time\n",
        "\n",
        "X_full = np.concatenate([X_train, X_test, X_val], axis=0)\n",
        "y_full = np.concatenate([y_train, y_test, y_val], axis=0)\n",
        "\n",
        "cvscore = cross_val_score(stacked_models, X_full, y_full, cv=StratifiedKFold(10), scoring='roc_auc_ovr').mean()\n",
        "\n",
        "estimators_model = pd.DataFrame()\n",
        "estimators_model['Models'] = ['Stacking Ensemble Model']\n",
        "estimators_model['ROC AUC'] = [roc_auc]\n",
        "estimators_model['ROC AUC Validation'] = [roc_auc_val]\n",
        "estimators_model['F1-score (macro)'] = [fscore_macro]\n",
        "estimators_model['F1-score (macro) Validation'] = [fscore_macro_val]\n",
        "estimators_model['F1-score (weighted)'] = [fscore_weighted]\n",
        "estimators_model['F1-score (weighted) Validation'] = [fscore_weighted_val]\n",
        "estimators_model['Precision'] = [precision]\n",
        "estimators_model['Precision Validation'] = [precision_val]\n",
        "estimators_model['Recall'] = [recall]\n",
        "estimators_model['Recall Validation'] = [recall_val]\n",
        "estimators_model['Recall Severe'] = [recall_severe]\n",
        "estimators_model['Recall Severe Validation'] = [recall_severe_val]\n",
        "estimators_model['Score'] = [score]\n",
        "estimators_model['Score Validation'] = [score_val]\n",
        "estimators_model['CV Score'] = [cvscore]\n",
        "estimators_model['Training Time'] = [processing_time]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c13b3333",
      "metadata": {
        "id": "c13b3333",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "print(estimators_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a4c11e2",
      "metadata": {
        "id": "6a4c11e2",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='g', cmap='Blues')\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.savefig('confusion_matrix.png', dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DodCQPzU-dw7",
      "metadata": {
        "id": "DodCQPzU-dw7"
      },
      "outputs": [],
      "source": [
        "for data_path in data_paths:\n",
        "    data_type = data_path.split('/')[1]\n",
        "    print(\"Data type:\", data_type)\n",
        "    output_dir = os.path.join('covid_baseline_classifier_outputs', data_type)\n",
        "    X_train, y_train, X_test, y_test, X_val, y_val = load_pre_split_data(data_type)\n",
        "    model, explainer_type, shap_filename, lime_filename, cm_filename, output_file_name, index = load_model(data_type)\n",
        "    train_and_explain_model(model, X_train, y_train, X_test, y_test, explainer_type, shap_filename, lime_filename, cm_filename, output_dir, output_file_name, index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vDTu_CBn-eyX",
      "metadata": {
        "id": "vDTu_CBn-eyX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "data = pd.read_excel('/content/met_feature_importance_class_2.xlsx')\n",
        "\n",
        "data = data.sort_values(by='importance', ascending=True).head(15)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(data['importance'], data['feature'], color='purple', s=100)\n",
        "\n",
        "plt.title('Importance of Plasma Metabolites for Severe Class')\n",
        "plt.xlabel('Derived Importance Score')\n",
        "plt.ylabel('Plasma Metabolites')\n",
        "\n",
        "plt.xlim([0, data['importance'].max() * 1.1])\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "plt.savefig('met_severe_class_importance_plot_top15.png', dpi=300, bbox_inches='tight')\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
